{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "object_detection_yolov4_custom_model_deploy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF5rGAnOF1Gm"
      },
      "source": [
        "# Object Detection using the YOLO V4 custom-trained model\n",
        "\n",
        "*by Georgios K. Ouzounis, June 10th, 2021*\n",
        "\n",
        "In this exercise we will experiment with object detection in still images using the YOLO V4 custom trained model for detecting face masks. Note  that  this is not a face detector. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB9dN1Io78DA"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXzDYD5Jqxdo"
      },
      "source": [
        "# import the relevant libraries\n",
        "import numpy as np\n",
        "import cv2 # openCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewrJSoXrGadW"
      },
      "source": [
        "# check the opencv version\n",
        "print(cv2.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmXkC427Gkqx"
      },
      "source": [
        "# if the openCV version is < 4.4.0 update to the latest otherwise skip this step\n",
        "!pip install opencv-python==4.5.2.52"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ms4LYX18B-1"
      },
      "source": [
        "## Get the model\n",
        "\n",
        "mount your Google Drive!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1TwKBXhHcXu"
      },
      "source": [
        "# first create a directory to store the model\n",
        "%mkdir model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY4WZZF1H1nZ"
      },
      "source": [
        "# enter the directory and download the necessary files \n",
        "%cd model\n",
        "%cp /content/drive/MyDrive/my_model/backup/yolov4_best.weights .\n",
        "%cp /content/drive/MyDrive/my_model/yolov4.cfg .\n",
        "%cp /content/drive/MyDrive/my_model/face_mask_classes.names .\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0u02442T7Bk"
      },
      "source": [
        "##  Get the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1ocmIkmUDRP"
      },
      "source": [
        "# create a directory for test data\n",
        "%mkdir test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C5Qqhvg8Mpg"
      },
      "source": [
        "# copy a test file locally or use it directly from the mounted drive \n",
        "# customize this example for your drive structure\n",
        "%cp /content/drive/MyDrive/data/object_detection/test_gko_rs.jpg test_data/\n",
        "%cp /content/drive/MyDrive/data/object_detection/test_gko_rs_ground_truth.txt test_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlGZYGMXWo_0"
      },
      "source": [
        "## Read test image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjmJxTYQ-06i"
      },
      "source": [
        "# read file\n",
        "test_img = cv2.imread('test_data/test_gko_rs.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mge8rb5CCq1z"
      },
      "source": [
        "# display test image\n",
        "# import the cv2_imshow as a replacement of cv2.imshow to prevent errors\n",
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow(test_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rALpx_t2YUN5"
      },
      "source": [
        "## Image2Blob\n",
        "\n",
        "convert the image to blob for model compatibility using the opencv builtin  dnn.blobFromImage() method\n",
        "\n",
        "Argument explanations:\n",
        "- scalefactor: multiplication factor for each pixel to rescale its intensity in  the range of [0,1]. No contrast stretching is performed. It is set to 1/255.0 = 0.003922.\n",
        "- new_size: rescaling size for network compatibility. We use (416, 416). Other supported sizes are (320, 320) and (609, 609). The greater the size is the better the prediction accuracy will be but at the cost of higher computational load.\n",
        "- swapRB: binary flag that if set instructs opencv to swap the red and blue channels. That is because opencv stores images in a BGR order but YOLO requires them in RGB.\n",
        "- crop: binary flag that if set instructs opencv to crop the image after resizing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGYR5dKX_fsJ"
      },
      "source": [
        "scalefactor = 1.0/255.0\n",
        "new_size = (416, 416)\n",
        "blob = cv2.dnn.blobFromImage(test_img, scalefactor, new_size, swapRB=True, crop=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRoo5k-QdB5K"
      },
      "source": [
        "## Customize the YOLO detector\n",
        "\n",
        "class labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvPiC9uxKrHk"
      },
      "source": [
        "class_labels_path = \"/content/model/face_mask_classes.names\"\n",
        "class_labels = open(class_labels_path).read().strip().split(\"\\n\")\n",
        "class_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJu-efpG4f8X"
      },
      "source": [
        "bounding box color definitions: two options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80wm8OWX_oxr"
      },
      "source": [
        "# declare repeating bounding box colors for each class \n",
        "# 1st: create a list colors as an RGB string array\n",
        "# Example: Red, Green,\n",
        "class_colors = [\"255,0,0\",\"0,255,0\"]\n",
        "\n",
        "#2nd: split the array on comma-seperated strings and for change each string type to integer\n",
        "class_colors = [np.array(every_color.split(\",\")).astype(\"int\") for every_color in class_colors]\n",
        "\n",
        "#3d: convert the array or arrays to a numpy array\n",
        "class_colors = np.array(class_colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHo_MgL40uma"
      },
      "source": [
        "## Load and run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBTX_4xYAhZc"
      },
      "source": [
        "# Load the pre-trained model \n",
        "yolo_model = cv2.dnn.readNetFromDarknet('model/yolov4.cfg','model/yolov4_best.weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cew-LCPDJOR"
      },
      "source": [
        "# Read the network layers/components. The YOLO V4 neural network has 379 components.\n",
        "# They consist of convolutional layers (conv), rectifier linear units (relu) etc.:\n",
        "model_layers = yolo_model.getLayerNames()\n",
        "print(\"number of network components: \" + str(len(model_layers))) \n",
        "print(model_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkBIXXd9E_lK"
      },
      "source": [
        "# Loop through all network layers to find the output layers\n",
        "output_layers = [model_layers[model_layer[0] - 1] for model_layer in yolo_model.getUnconnectedOutLayers()]\n",
        "\n",
        "# YOLOv4 deploys the same YOLO head as YOLOv3 for detection with   \n",
        "# the anchor based detection steps, and three levels of detection granularity. \n",
        "print(output_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZaZEi9mDKld"
      },
      "source": [
        "# input pre-processed blob into the model\n",
        "yolo_model.setInput(blob)\n",
        "\n",
        "# compute the forward pass for the input, storing the results per output layer in a list\n",
        "obj_detections_in_layers = yolo_model.forward(output_layers)\n",
        "\n",
        "# verify the number of sets of detections\n",
        "print(\"number of sets of detections: \" + str(len(obj_detections_in_layers)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6hPes5MTnHc"
      },
      "source": [
        "## Analyze the results\n",
        "\n",
        "The objective now is to get each object detection from each output layer and evaluate  the algorithm's cofidence score against a threshold. For high confidence detections we extract the class ID and the bounding box info."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXYCReuy5e7Q"
      },
      "source": [
        "## Non Maxima Suppression (NMS)\n",
        "\n",
        "supress fully or partially overlapping bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToQ-UBgYG6xw"
      },
      "source": [
        "%cp /content/drive/MyDrive/object_detection_functions.py ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFO4LdQiHtE2"
      },
      "source": [
        "from object_detection_functions import object_detection_analysis_with_nms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bb8nRNLOTRU"
      },
      "source": [
        "score_threshold = 0.5\n",
        "nms_threshold = 0.4\n",
        "result, winner_boxes = object_detection_analysis_with_nms(test_img, class_labels, class_colors, obj_detections_in_layers, score_threshold, nms_threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI9ddjWhSWAr"
      },
      "source": [
        "cv2_imshow(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM5t6MqX_V4p"
      },
      "source": [
        "create a 2D list containing the bounding box end points for each detection.\n",
        "\n",
        "create a 2D list containing the ground truth box end points for each object.\n",
        "\n",
        "both lists must be of the same dimensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1-3lgTgZ6hb"
      },
      "source": [
        "import io\n",
        "\n",
        "ground_truth_boxes = []\n",
        "\n",
        "with io.open(\"test_data/test_gko_rs_ground_truth.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "    ground_truth_boxes.append(line.split())\n",
        "\n",
        "for i in range(0, len(ground_truth_boxes)):\n",
        "  for j in range(0, len(ground_truth_boxes[i])):\n",
        "    ground_truth_boxes[i][j] = int(ground_truth_boxes[i][j])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7i2fcyNpIdO"
      },
      "source": [
        "### Compute the IoU metric\n",
        "\n",
        "The green bounding boxes depict the detected object while the red, the ground truth object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnfeR3OltcDn"
      },
      "source": [
        "from object_detection_functions import object_detection_iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS38yg2IkiVp"
      },
      "source": [
        "# create a copy of the test image\n",
        "iou_image = test_img.copy()\n",
        "\n",
        "# print the ground truth and detection bounding boxes, and the IoU value\n",
        "iou_image, iou_value = object_detection_iou(test_img, winner_boxes[0], ground_truth_boxes[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzUgZ0zweV4X"
      },
      "source": [
        "cv2_imshow(iou_image)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}